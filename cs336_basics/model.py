#import torch.nn.functional as F
import timeit
import torch
from typing import Iterable
from torch import nn
from torch import Tensor
import numpy as np
from typing import List
from einops import rearrange, einsum, reduce, repeat
import math
from jaxtyping import Bool, Float, Int

class TransformerLM(nn.Module):
    def __init__(self,
        vocab_size: int,
        context_length: int, #`sequence_length` is at most `context_length`.
        d_model: int,
        num_layers: int,
        num_heads: int,
        d_ff: int,
        rope_theta: float
    ) -> None:
        super().__init__()
        self.vocab_size = vocab_size
        self.max_seq_len = context_length
        self.d_ff = d_ff
        self.num_head = num_heads
        self.theta = rope_theta
        self.num_layers = num_layers
        self.d_model = d_model

        self.token_embeddings = Embedding(num_embeddings=vocab_size,embedding_dim=d_model)
        self.layers = nn.ModuleList([
            TransformerBlock(d_model,num_heads,d_ff,d_model,d_model,d_in=d_model,max_seq_len=context_length,theta=rope_theta)
            for _ in range(num_layers)
        ])
        self.ln_final = RMSNorm(d_model)
        self.lm_head = Linear(in_features=d_model, out_features=vocab_size)

    def forward(self, in_indices: Int[Tensor, " batch_size sequence_length"]) -> Float[Tensor, " batch_size sequence_length vocab_size"]:
        x = self.token_embeddings.forward(in_indices)
        for i in range(self.num_layers):
            x = self.layers[i].forward(x)
        x = self.ln_final.forward(x)
        x = self.lm_head.forward(x)
        # x = softmax(x, -1) # æ³¨æ„åœ¨adapterä¸­ï¼Œè¦æ±‚çš„æ˜¯æ²¡æœ‰æ­£åˆ™åŒ–çš„next-wordæ¦‚ç‡è¾“å‡º
        return x

class TransformerBlock(nn.Module):
    def __init__(self, 
        d_model: int, 
        num_heads: int, 
        d_ff: int, 
        d_k:int, 
        d_v:int,
        d_in:int,
        max_seq_len: int, 
        theta: float
    ):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_kh = d_model // num_heads
        self.d_vh = d_model // num_heads
        self.theta = theta
        self.max_seq_len = max_seq_len
        self.d_k = d_k
        self.d_v = d_v
        self.d_in = d_in

        self.attn = nn.Module()
        self.attn = MultiheadSelfAttention(
            d_model=d_model,
            num_heads=num_heads,
            d_k=d_k,
            d_v=d_v,
            d_in=d_in,
            with_rope=True,
            max_seq_len=max_seq_len,
            theta=theta
        )
        self.ln1 = nn.Module()
        self.ln1 = RMSNorm(d_model=d_model)

        self.ln2 = nn.Module()
        self.ln2 = RMSNorm(d_model=d_model)

        self.ffn = nn.Module()  # å¤–å±‚å­æ¨¡å—ï¼šffn
        self.ffn = SwiGLU(d_model=d_model,d_ff=d_ff)


    def forward(
        self,
        in_features: Float[Tensor, " batch sequence_length d_model"]
    )-> Float[Tensor, " batch sequence_length d_model"]:
        x1 = self.ln1.forward(in_features)
        x2 = self.attn.forward(x1)
        x3 = in_features + x2
        x4 = self.ln2.forward(x3)
        x5 = self.ffn.forward(x4)
        out = x3 + x5
        return out

class MultiheadSelfAttention(nn.Module):
    def __init__(
        self,
        d_model: int,
        num_heads: int,
        d_k: int,
        d_v: int,
        d_in: int,
        with_rope: bool = False,
        max_seq_len: int = -1,
        theta: float = 0.0,
    ):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_kh = d_model // num_heads  # å•å¤´ QK ç»´åº¦
        self.d_vh = d_model // num_heads  # å•å¤´ V ç»´åº¦

        # çº¿æ€§æŠ•å½±ï¼šæ³¨æ„ Linear çš„å®šä¹‰æ˜¯ (in_features, out_features)ï¼Œ
        # ä½ è¿™é‡Œçš„ Linear åº”è¯¥æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œå…¶ weight å½¢çŠ¶æ˜¯ [out, in]
        self.q_proj = Linear(d_k, d_in)  # [d_k, d_in]
        self.k_proj = Linear(d_k, d_in)
        self.v_proj = Linear(d_v, d_in)
        self.output_proj = Linear(d_model, d_v)

        # æ˜¾å¼åˆå§‹åŒ–ï¼ˆå¯ä»¥æŒ‰éœ€è¦æ¢åˆå§‹åŒ–æ–¹å¼ï¼‰
        self.q_proj.weight = nn.Parameter(torch.randn(d_k, d_in))
        self.k_proj.weight = nn.Parameter(torch.randn(d_k, d_in))
        self.v_proj.weight = nn.Parameter(torch.randn(d_v, d_in))
        self.output_proj.weight = nn.Parameter(torch.randn(d_model, d_v))

        self.theta = theta
        self.max_seq_len = max_seq_len
        self.with_rope = with_rope

        if self.with_rope:
            # RoPE ä½œç”¨åœ¨å•å¤´ç»´åº¦ d_kh ä¸Š
            self.rope = RotaryPositionalEmbedding(
                theta=self.theta,
                d_k=self.d_kh,
                max_seq_len=self.max_seq_len,
            )

    def forward(
        self,
        in_features: Float[Tensor, " ... sequence_length d_in"],
        token_positions: Int[Tensor, " ... sequence_length"] | None = None,
    ) -> Float[Tensor, " ... sequence_length d_out"]:
        """
        in_features: (..., seq_len, d_in)
        token_positions: (..., seq_len) æˆ– None
        """
        device = in_features.device
        seq_len = in_features.size(-2)
        if token_positions is not None:
            assert seq_len == token_positions.size(-1)

        d_in = in_features.size(-1)
        # ä½ å¯ä»¥åŠ ä¸€ä¸ªæ–­è¨€ï¼Œç¡®ä¿é…ç½®åˆç†ï¼š
        # assert d_in == self.d_model, "æœŸæœ› d_in == d_model"

        # è®¡ç®— Q, K, V
        Q = einsum(
            self.q_proj.weight,
            in_features,
            "d_k d_in, ... seq_len d_in -> ... seq_len d_k",
        )
        Q = rearrange(Q, "... seq_len (h d_kh) -> ... h seq_len d_kh", h=self.num_heads)

        K = einsum(
            self.k_proj.weight,
            in_features,
            "d_k d_in, ... seq_len d_in -> ... seq_len d_k",
        )
        K = rearrange(K, "... seq_len (h d_kh) -> ... h seq_len d_kh", h=self.num_heads)

        V = einsum(
            self.v_proj.weight,
            in_features,
            "d_v d_in, ... seq_len d_in -> ... seq_len d_v",
        )
        V = rearrange(V, "... seq_len (h d_vh) -> ... h seq_len d_vh", h=self.num_heads)

        # æå– batch-like ç»´åº¦ï¼ˆå¦‚ batch, num_heads, ä»¥åŠå¯èƒ½çš„å‰ç½®ç»´åº¦ï¼‰
        *batch_dims, _, _ = Q.shape  # (..., h, seq_len, d_kh)
        batchlike_str_dict = {f"batchlike{i}": dim for i, dim in enumerate(batch_dims)}
        batchlike_str = " ".join([f"batchlike{i}" for i in range(len(batch_dims))])

        # æ„é€  token_positionsï¼šç»Ÿä¸€åœ¨ in_features.device ä¸Š
        if token_positions is None:
            token_pos_1D = torch.arange(seq_len, device=device)  # (seq_len,)
            token_positions = repeat(
                token_pos_1D,
                "seq_len -> " + batchlike_str + " seq_len",
                **batchlike_str_dict,
            )
        else:
            # è¿ç§»åˆ°æ­£ç¡®çš„ device + dtype
            if token_positions.dtype != torch.long:
                token_positions = token_positions.long()
            token_positions = token_positions.to(device)

        # RoPEï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.with_rope:
            rope_Q = self.rope(Q, token_positions)
            rope_K = self.rope(K, token_positions)
        else:
            rope_Q, rope_K = Q, K

        # ç”Ÿæˆä¸‹ä¸‰è§’æ©ç ï¼šTrue=å…è®¸æ³¨æ„åŠ›ï¼ŒFalse=è¢«å±è”½
        base_mask = torch.tril(
            torch.ones((seq_len, seq_len), dtype=torch.bool, device=device),
            diagonal=0,
        )  # [seq, seq]

        mask = repeat(
            base_mask,
            "q k -> " + batchlike_str + " q k",
            **batchlike_str_dict,
        )  # (..., h, seq, seq)

        # è°ƒç”¨ scaled dot-product attention
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ scaled_dot_product_attention è¿”å›çš„æ˜¯è¾“å‡º valueï¼Œ
        # å¦‚æœä½ çš„å®ç°è¿”å› (out, attn_weights)ï¼Œè¿™é‡Œè¦ unpack ä¸€ä¸‹ã€‚
        attn_out = scaled_dot_product_attention(
            rope_Q, rope_K, V, mask=mask
        )  # (..., h, seq, d_vh)

        # å¦‚æœä½ çš„ scaled_dot_product_attention å®é™…å†™çš„æ˜¯ï¼š
        #     return out, attn
        # é‚£è¿™é‡Œè¦æ”¹ä¸ºï¼š
        # attn_out, attn_weights = scaled_dot_product_attention(...)

        # åˆå¹¶ heads
        attention_merged = rearrange(
            attn_out, "... h seq d_vh -> ... seq (h d_vh)"
        )  # (..., seq, d_v)

        out = einsum(
            self.output_proj.weight,
            attention_merged,
            "d_model d_v, ... d_v -> ... d_model",
        )
        return out

class Linear(nn.Module):
    def __init__(
        self, 
        in_features: int, 
        out_features: int, 
        device: torch.device | None = None, 
        dtype: torch.dtype | None = None
    ):
        super().__init__()
        std = math.sqrt(2.0 / (in_features + out_features))
        self.weight = nn.Parameter(torch.empty(out_features, in_features, device=device, dtype=dtype))
        nn.init.trunc_normal_(self.weight, mean = 0, std = std, a = -3 * std, b = 3 * std)

        self.in_features = in_features
        self.out_features = out_features
        self.device = device
        self.dtype = dtype

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.matmul(x, self.weight.T)

class Embedding(nn.Module):
    def __init__(
        self, 
        num_embeddings: int, # i.e., vocab_size
        embedding_dim: int, # i.e., dmodel
        device: torch.device | None = None, 
        dtype: torch.dtype | None = None 
    ):
        '''num_embeddings: Size of the vocabulary 
        embedding_dim: Dimension of the embedding vectors,   
        device: Device to store the parameters on  
        dtype: Data type of the parameters  '''
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.device = device
        self.dtype = dtype
        std = 1
        self.weight = nn.Parameter(
            torch.empty(num_embeddings, embedding_dim, device=device, dtype=dtype)
        )
        nn.init.trunc_normal_(self.weight, mean = 0, std = std, a = -3, b = 3)

    
    def forward(
        self, 
        token_ids: torch.Tensor
    ) -> torch.Tensor:
        '''Lookup the embedding vectors for the given token IDs.'''
        # return type (batch_size, sequence_length)
        # åŸå§‹ token_ids çš„å½¢çŠ¶æ˜¯ (B, T)ï¼ˆB=batch_sizeï¼ŒT=sequence_lengthï¼‰ï¼Œæ›¿æ¢åæ¯ä¸ªä½ç½®ä»æ ‡é‡ï¼ˆIDï¼‰å˜æˆäº†å‘é‡ï¼ˆd_model ç»´ï¼‰ï¼Œå› æ­¤æ•´ä½“å½¢çŠ¶æ‰©å±•ä¸º (B, T, d_model)ã€‚
        # è¾“å…¥ token_ids æ˜¯å½¢çŠ¶ä¸º (batch_size, sequence_length) çš„äºŒç»´å¼ é‡ï¼Œä¾‹å¦‚ (32, 10)ï¼ˆ32 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ 10 ä¸ª tokenï¼‰ã€‚
        # å½“æ‰§è¡Œ self.embedding_matrix[token_ids] æ—¶ï¼ŒPyTorch ä¼šæŒ‰ä»¥ä¸‹é€»è¾‘å¤„ç†ï¼š
        # å¯¹ token_ids ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆå³æ¯ä¸ª token IDï¼‰ï¼Œä»åµŒå…¥çŸ©é˜µä¸­å–å‡ºå¯¹åº”çš„è¡Œå‘é‡ï¼ˆå½¢çŠ¶ (d_model,)ï¼‰ã€‚
        # ä¿æŒ token_ids è‡ªèº«çš„ç»´åº¦ç»“æ„ä¸å˜ï¼Œä»…å°†æ¯ä¸ªå…ƒç´ æ›¿æ¢ä¸ºå¯¹åº”çš„åµŒå…¥å‘é‡ã€‚
        return self.weight[token_ids]

class RMSNorm(nn.Module):
    def __init__(
        self, 
        d_model: int, # Hidden dimension of the model
        eps: float = 1e-5, 
        device: torch.device | None = None, 
        dtype: torch.dtype | None = None
    ):
        super().__init__()
        self.d_model = d_model
        self.eps = eps
        self.device = device
        self.dtype = dtype
        self.weight = nn.Parameter(torch.empty(d_model, device=device, dtype=dtype))
        nn.init.ones_(self.weight)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        '''Process an input tensor of shape (batch_size, sequence_length, d_model) and return a tensor of the same shape.'''
        in_dtype = x.dtype
        x = x.to(torch.float32)
        # performing RMSNorm ... 
        mean_square = reduce(x ** 2, 'batch_size sequence_length d_model -> batch_size sequence_length 1', 'mean' ) # è¡¨ç¤ºç”¨æ±‚å¹³å‡çš„æ–¹å¼å‹ç¼©d_modelè¿™ä¸€ç»´, ä¸èƒ½ç›´æ¥æ¶ˆå»è¿™ä¸€ç»´ï¼Œå¦åˆ™ä¸‹ä¸€æ­¥ä¸åŒ¹é…
        rms = torch.sqrt(mean_square + self.eps) # Python æ ‡å‡†åº“çš„ math.sqrt æ˜¯ä¸ºå•ä¸ªæ•°å€¼è®¾è®¡çš„ï¼Œåªèƒ½æ¥æ”¶ä¸€ä¸ªæ ‡é‡ï¼ˆå¦‚ 3.14ï¼‰ï¼Œæ— æ³•å¤„ç† PyTorch å¼ é‡ï¼ˆå³ä½¿æ˜¯å•å…ƒç´ å¼ é‡ï¼‰ã€‚æ‰€ä»¥è¿™é‡Œä¸èƒ½ä½¿ç”¨math.sqrt; PyTorch çš„ torch.sqrt æ˜¯ä¸ºå¼ é‡è®¾è®¡çš„ï¼Œæ”¯æŒå¯¹å¼ é‡ä¸­çš„æ¯ä¸ªå…ƒç´ é€å…ƒç´ è®¡ç®—å¹³æ–¹æ ¹ï¼Œä¸”ä¿ç•™å¼ é‡çš„å½¢çŠ¶ã€‚
        result = x * self.weight / rms
        # Return the result in the original dtype 
        return result.to(in_dtype)

class SwiGLU(nn.Module):
    def __init__(
        self, 
        d_model: int,

        d_ff: int | None = None,  # å…è®¸æ˜¾å¼ä¼ å…¥d_ffï¼Œæµ‹è¯•æ—¶ä½¿ç”¨
        device: torch.device | None = None, 
        dtype: torch.dtype | None = None
    ):
        super().__init__()
        self.d_model = d_model
        self.device = device
        self.dtype = dtype
        # è‹¥æœªæ˜¾å¼ä¼ å…¥d_ffï¼Œåˆ™è‡ªåŠ¨è®¡ç®—ï¼š(8/3)*d_model å¹¶å‘ä¸Šå–æ•´ä¸º64çš„å€æ•°
        if d_ff is None:
            d_ff_candidate = (8 / 3) * d_model
            # å‘ä¸Šå–æ•´åˆ°æœ€è¿‘çš„64çš„å€æ•°
            d_ff = ((math.ceil(d_ff_candidate / 64)) * 64)
        self.d_ff = d_ff

        self.w1 = Linear(in_features=d_model, out_features=d_ff) # æ ¹æ®linearç±»çš„å®šä¹‰ï¼Œåº”è¯¥æ˜¯doutåœ¨å‰
        self.w2 = Linear(in_features=d_ff, out_features=d_model)
        self.w3 = Linear(in_features=d_model, out_features=d_ff)
        self.w1.weight = nn.Parameter(torch.empty((d_ff, d_model), device=device, dtype=dtype))
        self.w2.weight = nn.Parameter(torch.empty((d_model, d_ff), device=device, dtype=dtype))
        self.w3.weight = nn.Parameter(torch.empty((d_ff, d_model), device=device, dtype=dtype))
        nn.init.trunc_normal_(self.w1.weight, mean=0, std=1, a=-3, b=3)
        nn.init.trunc_normal_(self.w2.weight, mean=0, std=1, a=-3, b=3)
        nn.init.trunc_normal_(self.w3.weight, mean=0, std=1, a=-3, b=3)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        W1x = einsum(x, self.w1.weight.T,'... d_model, d_model d_ff-> ... d_ff')
        SiLU_of_W1x = W1x * torch.sigmoid(W1x)
        W3x = einsum(x, self.w3.weight.T,'... d_model, d_model d_ff-> ... d_ff')
        SiLU_of_W1x_times_W3x = SiLU_of_W1x * W3x
        res = einsum(SiLU_of_W1x_times_W3x, self.w2.weight.T, '... d_ff, d_ff d_model -> ... d_model')
        return res

# class RotaryPositionalEmbedding(nn.Module):
#     def __init__(
#         self, 
#         theta: float,
#         d_k: int,  # query/key çš„ç»´åº¦
#         max_seq_len: int,  # æœ€å¤§åºåˆ—é•¿åº¦
#         device=None
#     ):
#         super().__init__()
#         self.theta = theta
#         self.d_k = d_k
#         self.max_seq_len = max_seq_len
#         self.device = device or torch.device('cpu')

#         # ç¡®ä¿ d_k æ˜¯å¶æ•°ï¼ˆRoPE è¦æ±‚ç‰¹å¾æŒ‰å¯¹å¤„ç†ï¼‰
#         d = d_k if d_k % 2 == 0 else d_k + 1  
#         self.d = d

#         # æ­¥éª¤1ï¼šè®¡ç®—åˆ†æ¯ï¼ˆÎ¸^(2k-2)/dï¼‰
#         base = torch.full((1, d // 2), theta, device=self.device)
#         exp = torch.linspace(0, d - 2, d // 2, device=self.device) / d
#         denominator = torch.pow(base, exp)

#         # æ­¥éª¤2ï¼šè®¡ç®—åˆ†å­ï¼ˆä½ç½® iï¼‰
#         numerator = torch.arange(0, max_seq_len, device=self.device)

#         # æ­¥éª¤3ï¼šè®¡ç®— Î¸_ik = i / åˆ†æ¯ â†’ å½¢çŠ¶: (max_seq_len, d//2)
#         theta_ik = numerator.unsqueeze(1) / denominator
#         cos_theta = torch.cos(theta_ik)
#         sin_theta = torch.sin(theta_ik)
#         self.cos: Tensor
#         self.sin: Tensor
#         # æ³¨å†Œä¸º bufferï¼ˆéå¯å­¦ä¹ å‚æ•°ï¼‰
#         self.register_buffer('cos', cos_theta, persistent=False)
#         self.register_buffer('sin', sin_theta, persistent=False)

#     def forward(
#         self, 
#         in_query_or_key: torch.Tensor,  # å½¢çŠ¶: (... sequence_length d_k)
#         token_positions: torch.Tensor   # å½¢çŠ¶: (... sequence_length)
#     ) -> torch.Tensor:
#         """
#         å¯¹è¾“å…¥çš„æŸ¥è¯¢æˆ–é”®å¼ é‡åº”ç”¨ Rotary Position Embedding
        
#         å‚æ•°:
#             in_query_or_key: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (... sequence_length d_k)
#             token_positions: æ¯ä¸ª token çš„ä½ç½®ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (... sequence_length)
        
#         è¿”å›:
#             åº”ç”¨ RoPE åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ä¸€è‡´
#         """
#         # æå–ç»´åº¦ä¿¡æ¯
#         *batch_dims, seq_len, d_k = in_query_or_key.shape
#         assert d_k == self.d_k, "è¾“å…¥ç‰¹å¾ç»´åº¦ä¸åˆå§‹åŒ–æ—¶çš„ d_k ä¸åŒ¹é…"

#         # ç¡®ä¿ token_positions ä¸è¾“å…¥çš„ sequence_length ä¸€è‡´
#         assert token_positions.shape[-1] == seq_len, "token_positions çš„åºåˆ—é•¿åº¦ä¸è¾“å…¥ä¸åŒ¹é…"

#         # å¤„ç†è¾“å…¥ç»´åº¦ï¼ˆè‹¥ d_k æ˜¯å¥‡æ•°ï¼Œå…ˆè¡¥é›¶ä½¿ç»´åº¦ä¸ºå¶æ•°ï¼‰
#         if self.d != d_k:
#             in_padded = torch.nn.functional.pad(in_query_or_key, (0, self.d - d_k))
#         else:
#             in_padded = in_query_or_key

#         # æ‹†åˆ†ç‰¹å¾ç»´åº¦ä¸º (d//2, 2)ï¼ŒæŒ‰å¯¹å¤„ç†
#         in_reshaped = in_padded.reshape(*batch_dims, seq_len, self.d // 2, 2)

#         # æå–å½“å‰ token ä½ç½®å¯¹åº”çš„ cos å’Œ sinï¼ˆåˆ©ç”¨é«˜çº§ç´¢å¼•ï¼‰
#         # token_positions çš„å½¢çŠ¶æ˜¯ (... sequence_length)ï¼Œéœ€æ‰©å±•ä¸ºä¸ in_reshaped å…¼å®¹çš„å½¢çŠ¶
#         # å½“æ‰§è¡Œ self.cos[token_positions] æ—¶ï¼ŒPyTorch çš„é«˜çº§ç´¢å¼•ä¼šæ ¹æ® token_positions ä¸­çš„æ¯ä¸ªä½ç½®ç´¢å¼•ï¼Œä» self.cos ä¸­æå–å¯¹åº”è¡Œï¼Œæœ€ç»ˆå¾—åˆ°å½¢çŠ¶ä¸º (... sequence_length, d//2) çš„å¼ é‡ï¼Œå®ç° â€œæŒ‰ token ä½ç½®æå–æ—‹è½¬ç³»æ•°â€ çš„é€»è¾‘ã€‚
#         cos = self.cos[token_positions]  # å½¢çŠ¶: (... sequence_length d//2)
#         sin = self.sin[token_positions]  # å½¢çŠ¶: (... sequence_length d//2)

#         # åº”ç”¨æ—‹è½¬çŸ©é˜µï¼š[a, b] * [[cos, -sin], [sin, cos]] = [a*cos - b*sin, a*sin + b*cos]
#         a, b = in_reshaped[..., 0], in_reshaped[..., 1]
#         rotated_a = a * cos - b * sin
#         rotated_b = a * sin + b * cos

#         # é‡ç»„ä¸ºåŸå§‹ç»´åº¦
#         # æŠŠä¸¤ä¸ªå½¢çŠ¶ä¸º (..., seq, d//2) çš„å¼ é‡åˆå¹¶ä¸ºå½¢çŠ¶ (..., seq, d//2, 2) çš„å¼ é‡ï¼ˆæ¯ä¸ªç‰¹å¾å¯¹çš„ä¸¤ä¸ªåˆ†é‡åœ¨æœ€åä¸€ç»´èšåˆï¼‰
#         rotated = torch.stack([rotated_a, rotated_b], dim=-1)
#         rotated = rearrange(rotated, '... seq d2 two -> ... seq (d2 two)') # ä¸èƒ½æŠŠtwoæ”¹æˆ2
#         # rotated = rotated.reshape(*batch_dims, seq_len, self.d)
#         # ç›´æ¥å°† rotated_a å’Œ rotated_b æŒ‰æœ€åä¸€ç»´å †å ï¼Œå†é‡ç»„ç»´åº¦
#         # rotated = rearrange([rotated_a, rotated_b], ' ... seq d2 2 -> ... seq (d2 2)')

#         # è‹¥åŸå§‹ d_k æ˜¯å¥‡æ•°ï¼Œè£å‰ªå›åŸç»´åº¦
#         if self.d != d_k:
#             rotated = rotated[..., :d_k]

#         return rotated
class RotaryPositionalEmbedding(nn.Module):
    def __init__(
        self, 
        theta: float,
        d_k: int,        # query/key çš„ç»´åº¦
        max_seq_len: int # æœ€å¤§åºåˆ—é•¿åº¦
    ):
        super().__init__()
        self.theta = theta
        self.d_k = d_k
        self.max_seq_len = max_seq_len

        # ç¡®ä¿ d_k æ˜¯å¶æ•°ï¼ˆRoPE è¦æ±‚ç‰¹å¾æŒ‰å¯¹å¤„ç†ï¼‰
        d = d_k if d_k % 2 == 0 else d_k + 1  
        self.d = d

        # è¿™é‡Œé»˜è®¤åœ¨ CPU ä¸Šå»ºè¡¨ï¼Œä¹‹åé€šè¿‡ model.to(device) æˆ– forward é‡Œçš„ .to() è¿ç§»
        base = torch.full((1, d // 2), theta)               # [1, d//2]
        exp = torch.linspace(0, d - 2, d // 2) / d          # [d//2]
        denominator = torch.pow(base, exp)                  # [1, d//2]

        numerator = torch.arange(0, max_seq_len)            # [max_seq_len]

        # Î¸_ik = i / åˆ†æ¯ â†’ å½¢çŠ¶: (max_seq_len, d//2)
        theta_ik = numerator.unsqueeze(1) / denominator     # [max_seq_len, d//2]
        cos_theta = torch.cos(theta_ik)
        sin_theta = torch.sin(theta_ik)

        # æ³¨å†Œä¸º bufferï¼ˆéå¯å­¦ä¹ å‚æ•°ï¼‰ï¼Œè¿™æ · model.to(device) æ—¶ä¼šè‡ªåŠ¨è¿ç§»
        self.register_buffer('cos', cos_theta, persistent=False)  # [max_seq_len, d//2]
        self.register_buffer('sin', sin_theta, persistent=False)  # [max_seq_len, d//2]

    def forward(
        self, 
        in_query_or_key: torch.Tensor,  # å½¢çŠ¶: (..., seq_len, d_k)
        token_positions: torch.Tensor   # å½¢çŠ¶: (..., seq_len) æˆ– (seq_len,)
    ) -> torch.Tensor:
        """
        å¯¹è¾“å…¥çš„æŸ¥è¯¢æˆ–é”®å¼ é‡åº”ç”¨ Rotary Position Embedding
        
        å‚æ•°:
            in_query_or_key: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (..., seq_len, d_k)
            token_positions: æ¯ä¸ª token çš„ä½ç½®ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (..., seq_len) æˆ– (seq_len,)
        
        è¿”å›:
            åº”ç”¨ RoPE åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ä¸€è‡´
        """
        # æå–ç»´åº¦ä¿¡æ¯
        *batch_dims, seq_len, d_k = in_query_or_key.shape
        assert d_k == self.d_k, f"è¾“å…¥ç‰¹å¾ç»´åº¦ {d_k} ä¸åˆå§‹åŒ–æ—¶çš„ d_k={self.d_k} ä¸åŒ¹é…"

        # ç¡®ä¿ token_positions ä¸è¾“å…¥çš„ sequence_length ä¸€è‡´
        assert token_positions.shape[-1] == seq_len, (
            f"token_positions çš„åºåˆ—é•¿åº¦ {token_positions.shape[-1]} ä¸è¾“å…¥ {seq_len} ä¸åŒ¹é…"
        )

        device = in_query_or_key.device

        # token_positions éœ€è¦æ˜¯ longï¼Œå¹¶ä¸”æ”¾åˆ°å’Œè¾“å…¥åŒä¸€ä¸ª device ä¸Š
        if token_positions.dtype != torch.long:
            token_positions = token_positions.long()
        token_positions = token_positions.to(device)

        # å°† RoPE è¡¨ä¹Ÿè¿ç§»åˆ°è¾“å…¥æ‰€åœ¨è®¾å¤‡ï¼ˆå³ä½¿ä½ å¿˜è®° model.to(device)ï¼Œè¿™é‡Œä¹Ÿå…œåº•ï¼‰
        cos = self.cos
        sin = self.sin
        if cos.device != device:
            cos = cos.to(device)
            sin = sin.to(device)

        # å¤„ç†è¾“å…¥ç»´åº¦ï¼ˆè‹¥ d_k æ˜¯å¥‡æ•°ï¼Œå…ˆè¡¥é›¶ä½¿ç»´åº¦ä¸ºå¶æ•°ï¼‰
        if self.d != d_k:
            in_padded = torch.nn.functional.pad(in_query_or_key, (0, self.d - d_k))
        else:
            in_padded = in_query_or_key

        # æ‹†åˆ†ç‰¹å¾ç»´åº¦ä¸º (d//2, 2)ï¼ŒæŒ‰å¯¹å¤„ç†
        # in_reshaped: (..., seq_len, d//2, 2)
        in_reshaped = in_padded.reshape(*batch_dims, seq_len, self.d // 2, 2)

        # æå–å½“å‰ token ä½ç½®å¯¹åº”çš„ cos å’Œ sin
        # self.cos: [max_seq_len, d//2]
        # token_positions: (..., seq_len)
        # é«˜çº§ç´¢å¼•å cos/sin å½¢çŠ¶ä¸º (..., seq_len, d//2)
        cos = cos[token_positions]
        sin = sin[token_positions]

        # åº”ç”¨æ—‹è½¬çŸ©é˜µï¼š
        # [a, b] * [[cos, -sin], [sin, cos]] = [a*cos - b*sin, a*sin + b*cos]
        # in_reshaped: (..., seq_len, d//2, 2)
        a = in_reshaped[..., 0]  # (..., seq_len, d//2)
        b = in_reshaped[..., 1]  # (..., seq_len, d//2)

        # è¿™é‡Œ a/b ä¸ cos/sin å½¢çŠ¶ç›¸åŒï¼ŒæŒ‰å…ƒç´ ç›¸ä¹˜å³å¯
        rotated_a = a * cos - b * sin
        rotated_b = a * sin + b * cos

        # é‡ç»„ä¸ºåŸå§‹ç»´åº¦
        # rotated: (..., seq_len, d//2, 2)
        rotated = torch.stack([rotated_a, rotated_b], dim=-1)
        rotated = rearrange(rotated, '... seq d2 two -> ... seq (d2 two)')  # (..., seq_len, d)

        # è‹¥åŸå§‹ d_k æ˜¯å¥‡æ•°ï¼Œè£å‰ªå›åŸç»´åº¦
        if self.d != d_k:
            rotated = rotated[..., :d_k]

        return rotated



def get_device(index: int = 0) -> torch.device:
    """Try to use the GPU if possible, otherwise, use CPU."""
    if torch.cuda.is_available():
        return torch.device(f"cuda:{index}")
    else:
        return torch.device("cpu")

def scaled_dot_product_attention(
    Q: Float[Tensor, " ... queries d_k"],
    K: Float[Tensor, " ... keys d_k"],
    V: Float[Tensor, " ... keys d_v"],
    mask: Bool[Tensor, " ... queries keys"] | None = None,
) -> Float[Tensor, " ... queries d_v"]:
    d_k = Q.size(-1)
    pre_softmax = einsum(Q,K, '... queries d_k, ... keys d_k -> ... queries keys') / math.sqrt(d_k)
    
    if mask is not None:
        assert pre_softmax.shape == mask.shape
        #pre_softmax[~mask] = - torch.inf # ä½¿ç”¨Pytorchæ™ºèƒ½ç´¢å¼•ï¼Œåœ¨maské‡Œæ¯ä¸€ä¸ªä¸ºfalseçš„ä½ç½®ï¼Œpre_softmaxéƒ½å˜æˆè´Ÿæ— ç©·
        # æœ€å¥½é¿å…åŸåœ°æ“ä½œï¼Œè€Œæ”¹ç”¨mask_fill
        masked = pre_softmax.masked_fill(~mask, -torch.inf)
    else:
        masked = pre_softmax
    # Softmax ä½œç”¨äºÂ Q^T KÂ çš„æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå³Â mÂ ç»´åº¦ï¼Œå¯¹åº”é”®çš„åºåˆ—é•¿åº¦ï¼‰ï¼Œç›®çš„æ˜¯å¯¹æ¯ä¸ªæŸ¥è¯¢ï¼ˆnÂ ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼‰ï¼Œåœ¨æ‰€æœ‰é”®çš„ä½ç½®ä¸Šè®¡ç®—å½’ä¸€åŒ–çš„æ³¨æ„åŠ›æƒé‡ï¼ˆä½¿å¾—æ¯ä¸ªæŸ¥è¯¢å¯¹åº”çš„æƒé‡å’Œä¸º 1ï¼‰
    res = einsum(softmax(masked, dim=-1), V, '... queries keys , ... keys d_v -> ... queries d_v') 
    # maskedæ˜¯' ... queries keys'çš„ï¼Œå¯¹äºæœ€åä¸€ç»´æ±‚softmaxï¼Œå°±æ˜¯å¯¹ä»»æ„ä¸€ä¸ªqueryï¼Œè¾“å‡ºæ³¨æ„åŠ›éƒ½æ˜¯å½’ä¸€åŒ–çš„ã€‚Softmax å¯¹ Key ç»´ï¼ˆè€Œé Query ç»´ï¼‰å½’ä¸€åŒ–ï¼Œæ˜¯ä¸ºäº†è®©æ¯ä¸ª Query éƒ½èƒ½å¾—åˆ°ä¸€ä¸ªâ€˜é’ˆå¯¹æ‰€æœ‰ Key çš„å½’ä¸€åŒ–æƒé‡åˆ†â€™
    # value å¿…é¡»ç­‰äº keysï¼Œè¿™æ˜¯å› ä¸ºæ¯ä¸€ä¸ªqueryå¯¹åº”keysä¸ªé”®ï¼Œåšå†…ç§¯å°±æ˜¯ä¸ºäº†çŸ¥é“æŸä¸€ä¸ªqueryä¸æ‰€æœ‰é”®çš„ç›¸ä¼¼åº¦ï¼Œè¿›è€ŒæŒ‡å¯¼è¾“å‡ºæ³¨æ„åŠ›çš„åˆ†é…ï¼Œå³valueçš„åˆ†é…ï¼Œæ‰€ä»¥ä¸€ä¸ªkeyå°±å¯¹åº”ä¸€ä¸ªvalue
    return res

def softmax(
    in_features: Float[Tensor, " ..."],
    dim: int
    ) -> Float[Tensor, " ..."]:
    in_dtype = in_features.dtype
    in_features = in_features.to(torch.float32)
    max_entry = torch.max(in_features, dim=dim, keepdim=True).values # åº”è¯¥æ˜¯ä» '... dim ...' -> '... 1 ...'
    subtracted = torch.sub(in_features, max_entry)
    exp = torch.exp(subtracted)
    exp_sum = torch.sum(exp, dim=dim, keepdim=True)
    res = exp / exp_sum
    res = res.to(in_dtype)
    return res

def SiLU(in_features:Float[Tensor, "..."])->Float[Tensor,"..."]:
    return in_features * torch.sigmoid(in_features)


# æµ‹è¯•å‡½æ•°
# def test_swiglu_load_state_dict():
#     # 1. é…ç½®æµ‹è¯•å‚æ•°ï¼ˆæ¨¡æ‹Ÿ TransformerBlock ä¸­çš„åˆå§‹åŒ–å‚æ•°ï¼‰
#     d_model = 512  # æ¨¡å‹ç»´åº¦
#     d_ff = 1344    # ä¸ (8/3)*512=1365.333 å‘ä¸Šå–æ•´åˆ°64å€æ•°ä¸€è‡´ï¼ˆ1344æ˜¯64*21ï¼Œå®é™…å¯è°ƒæ•´ï¼Œæ­¤å¤„ä»…ä¸ºæµ‹è¯•ï¼‰
#     device = torch.device("cpu")  # å¯æ”¹ä¸º "cuda" æµ‹è¯•GPU
#     dtype = torch.float32

#     # 2. æ¨¡æ‹Ÿ TransformerBlock ä¸­ ffn æƒé‡åˆå§‹åŒ–
#     ffn = nn.Module()
#     # åˆå§‹åŒ– ffn.w1.weightï¼ˆå½¢çŠ¶ï¼š(d_model, d_ff) = (512, 1344)ï¼‰
#     ffn.w1 = nn.Module()
#     ffn.w1.weight = nn.Parameter(torch.randn(d_model, d_ff, device=device, dtype=dtype))  # ç”¨éšæœºå€¼æ¨¡æ‹Ÿæµ‹è¯•æƒé‡
#     # åˆå§‹åŒ– ffn.w2.weightï¼ˆå½¢çŠ¶ï¼š(d_ff, d_model) = (1344, 512)ï¼‰
#     ffn.w2 = nn.Module()
#     ffn.w2.weight = nn.Parameter(torch.randn(d_ff, d_model, device=device, dtype=dtype))
#     # åˆå§‹åŒ– ffn.w3.weightï¼ˆå½¢çŠ¶ï¼š(d_model, d_ff) = (512, 1344)ï¼‰
#     ffn.w3 = nn.Module()
#     ffn.w3.weight = nn.Parameter(torch.randn(d_model, d_ff, device=device, dtype=dtype))

#     # 3. æ¨¡æ‹Ÿ TransformerBlock ä¸­æ„å»º ffn_weightsï¼ˆé”®åï¼šw1_weight/w2_weight/w3_weightï¼‰
#     ffn_weights = {
#         'w1_weight': ffn.w1.weight.T,
#         'w2_weight': ffn.w2.weight.T,
#         'w3_weight': ffn.w3.weight.T,
#     }

#     # 4. å®ä¾‹åŒ– SwiGLU
#     swiglu = SwiGLU(
#         d_model=d_model,
#         d_ff=d_ff,  # æ˜¾å¼ä¼ å…¥d_ffï¼Œä¸ TransformerBlock ä¸€è‡´
#         device=device,
#         dtype=dtype
#     )

#     # 5. ä¿å­˜ SwiGLU åˆå§‹åŒ–æ—¶çš„æƒé‡ï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼ŒéªŒè¯æ˜¯å¦è¢«è¦†ç›–ï¼‰
#     init_w1 = swiglu.w1_weight.data.clone()
#     init_w2 = swiglu.w2_weight.data.clone()
#     init_w3 = swiglu.w3_weight.data.clone()

#     # 6. æ‰§è¡Œ load_state_dictï¼ˆæ ¸å¿ƒæµ‹è¯•æ­¥éª¤ï¼‰
#     try:
#         swiglu.load_state_dict(ffn_weights, strict=True)
#         print("âœ… load_state_dict æ‰§è¡ŒæˆåŠŸï¼ˆæ— é”®åä¸åŒ¹é…é”™è¯¯ï¼‰")
#     except Exception as e:
#         assert False, f"âŒ load_state_dict æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"

#     # 7. æ ¡éªŒæƒé‡åŠ è½½ç»“æœï¼ˆæ ¸å¿ƒæ ¡éªŒé¡¹ï¼‰
#     print("\n=== æƒé‡åŠ è½½æ ¡éªŒ ===")
#     # æ ¡éªŒ1ï¼šw1_weight è¢«æ­£ç¡®è¦†ç›–ï¼ˆä¸ ffn.w1.weight å®Œå…¨ä¸€è‡´ï¼‰
#     w1_match = torch.allclose(swiglu.w1_weight.data, ffn.w1.weight.T.data)
#     assert w1_match, "âŒ w1_weight åŠ è½½å¤±è´¥ï¼ˆä¸ ffn.w1.weight ä¸ä¸€è‡´ï¼‰"
#     print("âœ… w1_weight åŠ è½½æˆåŠŸ")

#     # æ ¡éªŒ2ï¼šw2_weight è¢«æ­£ç¡®è¦†ç›–ï¼ˆä¸ ffn.w2.weight å®Œå…¨ä¸€è‡´ï¼‰
#     w2_match = torch.allclose(swiglu.w2_weight.data, ffn.w2.weight.T.data)
#     assert w2_match, "âŒ w2_weight åŠ è½½å¤±è´¥ï¼ˆä¸ ffn.w2.weight ä¸ä¸€è‡´ï¼‰"
#     print("âœ… w2_weight åŠ è½½æˆåŠŸ")

#     # æ ¡éªŒ3ï¼šw3_weight è¢«æ­£ç¡®è¦†ç›–ï¼ˆä¸ ffn.w3.weight å®Œå…¨ä¸€è‡´ï¼‰
#     w3_match = torch.allclose(swiglu.w3_weight.data, ffn.w3.weight.T.data)
#     assert w3_match, "âŒ w3_weight åŠ è½½å¤±è´¥ï¼ˆä¸ ffn.w3.weight ä¸ä¸€è‡´ï¼‰"
#     print("âœ… w3_weight åŠ è½½æˆåŠŸ")

#     # æ ¡éªŒ4ï¼šåŠ è½½åçš„æƒé‡ä¸åˆå§‹åŒ–æƒé‡ä¸åŒï¼ˆç¡®ä¿ç¡®å®è¢«è¦†ç›–ï¼‰
#     init_w1_diff = not torch.allclose(swiglu.w1_weight.data, init_w1)
#     init_w2_diff = not torch.allclose(swiglu.w2_weight.data, init_w2)
#     init_w3_diff = not torch.allclose(swiglu.w3_weight.data, init_w3)
#     assert init_w1_diff and init_w2_diff and init_w3_diff, "âŒ æƒé‡æœªè¢«è¦†ç›–ï¼ˆä¸åˆå§‹åŒ–å€¼ä¸€è‡´ï¼‰"
#     print("âœ… åŠ è½½çš„æƒé‡æˆåŠŸè¦†ç›–åˆå§‹åŒ–æƒé‡")

#     # æ ¡éªŒ5ï¼šæƒé‡å½¢çŠ¶æ­£ç¡®ï¼ˆåŒ¹é… SwiGLU å®šä¹‰çš„å½¢çŠ¶ï¼‰
#     assert swiglu.w1_weight.shape == (d_ff, d_model), f"âŒ w1_weight å½¢çŠ¶é”™è¯¯ï¼šé¢„æœŸ ({d_ff}, {d_model})ï¼Œå®é™… {swiglu.w1_weight.shape}"
#     assert swiglu.w2_weight.shape == (d_model, d_ff), f"âŒ w2_weight å½¢çŠ¶é”™è¯¯ï¼šé¢„æœŸ ({d_model}, {d_ff})ï¼Œå®é™… {swiglu.w2_weight.shape}"
#     assert swiglu.w3_weight.shape == (d_ff, d_model), f"âŒ w3_weight å½¢çŠ¶é”™è¯¯ï¼šé¢„æœŸ ({d_ff}, {d_model})ï¼Œå®é™… {swiglu.w3_weight.shape}"
#     print("âœ… æ‰€æœ‰æƒé‡å½¢çŠ¶ç¬¦åˆé¢„æœŸ")

#     # 8. é¢å¤–æ ¡éªŒï¼šSwiGLU å‰å‘ä¼ æ’­æ­£å¸¸ï¼ˆç¡®ä¿åŠ è½½æƒé‡ååŠŸèƒ½ä¸å—å½±å“ï¼‰
#     test_input = torch.randn(2, 10, d_model, device=device, dtype=dtype)  # (batch, seq_len, d_model)
#     try:
#         output = swiglu(test_input)
#         assert output.shape == (2, 10, d_model), f"âŒ å‰å‘ä¼ æ’­è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼šé¢„æœŸ (2,10,{d_model})ï¼Œå®é™… {output.shape}"
#         print("âœ… å‰å‘ä¼ æ’­æ­£å¸¸ï¼Œè¾“å‡ºå½¢çŠ¶ç¬¦åˆé¢„æœŸ")
#     except Exception as e:
#         assert False, f"âŒ å‰å‘ä¼ æ’­å¤±è´¥ï¼š{str(e)}"

#     print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SwiGLU.load_state_dict åŠŸèƒ½æ­£å¸¸")

if __name__ == "__main__":
    batch_size = 4
    # vocab_size = 5
    # inputs = torch.rand(batch_size, vocab_size)
    # targets =  torch.randint(0,vocab_size-1,(batch_size,))

    # print(cross_entropy(1000*inputs, targets))
    